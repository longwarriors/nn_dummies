{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "\n",
    "def forward_linear_regression(X: np.array,\n",
    "                              y: np.array,\n",
    "                              weights: Dict[str, np.ndarray]) -> Tuple[float, Dict[str, np.ndarray]]:\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    assert X.shape[1] == weights['w'].shape[0]\n",
    "    assert weights['b'].shape[0] == weights['b'].shape[1] == 1\n",
    "    N = np.dot(X, weights['w'])\n",
    "    P = N + weights['b']\n",
    "    loss = np.square(y - P).mean()\n",
    "    forward_info: Dict[str, np.ndarray] = {'X': X, 'y': y, 'N': N, 'P': P}\n",
    "    return loss, forward_info\n",
    "\n",
    "\n",
    "def loss_gradient(forward_info: Dict[str, np.ndarray],\n",
    "                  weights: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    batch_size = forward_info['X'].shape[0]\n",
    "    dL_dP = -2 * (forward_info['y'] - forward_info['P'])\n",
    "    dP_dN = np.ones_like(forward_info['N'])\n",
    "    dP_dB = np.zeros_like(weights['b'])\n",
    "    dL_dN = dL_dP * dP_dN\n",
    "    dN_dW = np.transpose(forward_info['X'], (1, 0))\n",
    "    dL_dW = np.dot(dN_dW,dL_dN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
